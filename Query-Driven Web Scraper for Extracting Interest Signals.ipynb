{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5OirjJ9oko11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be00d4d-2a0c-44cd-ce9f-351ebb4ac735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import re\n",
        "import requests\n",
        "import tldextract\n",
        "import streamlit as st\n",
        "import io\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from rake_nltk import Rake\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### STREAMLIT INIT\n",
        "# Configure user inputs\n",
        "st.sidebar.title(\"Search Configuration\")\n",
        "\n",
        "serpapi_key = st.sidebar.text_input(\"Please input your SerpAPI Key\")\n",
        "if not serpapi_key:\n",
        "  st.sidebar.warning(\"Get serpAPI keys here https://serpapi.com/\")\n",
        "\n",
        "company_query = st.sidebar.text_input(\"What companies or topics are you searching for?\")\n",
        "\n",
        "interest_query = st.sidebar.text_input(\"Enter a keyword you're curious about (e.g., sustainability):\")\n",
        "\n",
        "num_query = st.sidebar.number_input(\"Number of Google Results to Fetch\", min_value=10, step= 10, value=20)\n",
        "\n",
        "st.title(\"Keywords & Trend Scraper\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "This tool will :\n",
        "1. Search companies using your SerpAPI key\n",
        "2. Extract domains and keywords\n",
        "3. Analyze how those websites discuss subject of interest\n",
        "\"\"\")\n",
        "\n",
        "# Debug for dev mode & validation\n",
        "st.write(\"**Entered SerpAPI Key:**\", serpapi_key[:4] + \"...\" if serpapi_key else \"\")\n",
        "st.write(\"**Search Query:**\", company_query)\n",
        "st.write(\"**Subject of Interest:**\", interest_query)\n",
        "st.write(\"**Number of Queries:**\", num_query)\n",
        "\n",
        "if serpapi_key and company_query and interest_query:\n",
        "  st.success(\"Ready to search and analyze!\")\n",
        "else:\n",
        "  st.warning(\"Please fill in all fields to begin\")"
      ],
      "metadata": {
        "id": "1zYzfw12RjT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataframe for web scraping result\n",
        "# Scrape from google and get the results\n",
        "def get_company_data_from_serpapi(company_query, serpapi_key, num_query=20):\n",
        "  df = pd.DataFrame(columns=[\n",
        "      'Company',\n",
        "      'Domain'\n",
        "  ]) # reinitialize data frame\n",
        "\n",
        "  for start in range(0, num_query, 10): #scrape amount\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": company_query,\n",
        "        \"start\": start,\n",
        "        \"api_key\": serpapi_key\n",
        "    }\n",
        "\n",
        "    response = requests.get(\"https://serpapi.com/search\", params=params)\n",
        "    results = response.json().get(\"organic_results\", [])\n",
        "\n",
        "  # Fill DataFrame\n",
        "    for res in results:\n",
        "      title = res.get(\"title\")\n",
        "      link = res.get(\"link\")\n",
        "      if title and link:\n",
        "        fill_df = pd.DataFrame([{\n",
        "            'Company': title,\n",
        "            'Domain': link\n",
        "        }])\n",
        "        df = pd.concat([df, fill_df], ignore_index= True)\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "QazvENdZoBFG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter third party / aggregator websites\n",
        "third_party_indicator = [\n",
        "    \"top\", \"best\", \"leading\", \"directory\", \"review\", \"compare\", \"list\",\n",
        "    \"ranking\", \"companies\", \"agencies\", \"firms\", \"vendors\", \"providers\",\n",
        "    \"expert\", \"consultant\", \"outsource\", \"services\", \"evaluations\", \"insights\",\n",
        "    \"buyers-guide\",\"blog\",\"wikipedia\",\"how\",\"developers\",\"linkedin\",\"work\",\"year\",\n",
        "    \"country\",\"what\",\"where\",\"who\",\"why\",\"when\",\"guide\",\"news\",\"research\",\"report\",\"insight\",\n",
        "    \"magazine\",\"travel\",\"looking\",\"fandom\",\"category\",\"directory\",\"news\",\"website\",\"journal\"\n",
        "    \"group\",\"agency\",\"paper\",\"article\",\"instagram\",\"facebook\",\"site\",\"tiktok\",\"video\",\n",
        "    \"youtube\",\"reddit\",\"quora\"\n",
        "]\n",
        "\n",
        "def filter_valid_company_sites(df):\n",
        "  def company_website(domain, company):\n",
        "    domain = str(domain).lower()\n",
        "    company = str(company).lower()\n",
        "\n",
        "    for tp in third_party_indicator:\n",
        "      if tp in domain or tp in company:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "  df = df[df.apply(lambda row: company_website(row['Company'], row['Domain']), axis =1)].reset_index(drop=True)\n",
        "  df = df.drop_duplicates(subset=['Company'], keep='first').reset_index(drop=True)\n",
        "  df = df.drop_duplicates(subset=['Domain'], keep='first').reset_index(drop=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "t3UGDZwdsNEv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace site Titles with exact company name for better presentation\n",
        "def get_company_name(url):\n",
        "  extracted = tldextract.extract(url)\n",
        "  domain_part = extracted.domain\n",
        "  return domain_part.capitalize()\n"
      ],
      "metadata": {
        "id": "yJFjoCy3xlsG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get trending words on websites\n",
        "def query_keywords(url, company_query, top_n=5):\n",
        "  try:\n",
        "    response = requests.get(url, timeout=10)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Get seen text\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "      script.extract()\n",
        "\n",
        "    # limit seen text to main only\n",
        "    main = soup.find('main') or soup.find('div', {'id': 'main'}) or soup.find('div', {'id': 'content'})\n",
        "    text = main.get_text(separator=' ') if main else soup.get_text(separator=' ')\n",
        "\n",
        "    # extract words using Rake nltk\n",
        "    rake = Rake()\n",
        "    rake.extract_keywords_from_text(text)\n",
        "    ranked_phrases = rake.get_ranked_phrases()\n",
        "\n",
        "    # Exclude words that are in query\n",
        "    query_words = set(company_query.lower().split())\n",
        "\n",
        "    filtered_indices = [\n",
        "        phrases for phrases in ranked_phrases\n",
        "        if not any(word in query_words for word in phrases.lower().split())\n",
        "        and len(phrases.strip().split()) >= 2\n",
        "    ]\n",
        "\n",
        "    selected_phrases = filtered_indices[:top_n] if filtered_indices else text[:top_n]\n",
        "\n",
        "    # Clean the phrase and put in a list for ease of process\n",
        "    all_words = set()\n",
        "    for phrase in selected_phrases:\n",
        "      words = phrase.lower().split()\n",
        "      if all(word.isalpha() for word in words):\n",
        "        all_words.add(' '.join(words))\n",
        "\n",
        "    return list(all_words)\n",
        "\n",
        "  # error for timeout limit\n",
        "  except requests.exceptions.ReadTimeout:\n",
        "    print(f\"{url} Unable to Load\")\n",
        "    return []\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error fetching {url}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ri5PIamQ6ftr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the output\n",
        "def remove_repeat(phrase, threshold= 0.5):\n",
        "  words = phrase.lower().split()\n",
        "  deduped = [words[0]] if words else []\n",
        "  for i in range(1, len(words)):\n",
        "    if words[i] != words[i-1]:\n",
        "      deduped.append(words[i])\n",
        "  return ' '.join(deduped)\n",
        "\n",
        "def analyze_keywords(keyword_df):\n",
        "  indexed_keywords = []\n",
        "  cleaned_keywords = []\n",
        "\n",
        "  for idx, keyword_list in keyword_df['Keywords'].items():\n",
        "    if isinstance(keyword_list, list):\n",
        "      for phrase in keyword_list:\n",
        "        indexed_keywords.append((idx, phrase))\n",
        "        cleaned = remove_repeat(phrase)\n",
        "        words = cleaned.lower().split()\n",
        "        if (\n",
        "            1 <= len(words) <= 8\n",
        "            and sum(w.isalpha() for w in words) >= len(words) - 1\n",
        "        ):\n",
        "          cleaned_keywords.append(cleaned)\n",
        "\n",
        "  unique_keywords = list(dict.fromkeys(cleaned_keywords))\n",
        "  keyword_df = pd.DataFrame(unique_keywords, columns=['Keyword'])\n",
        "\n",
        "  return keyword_df"
      ],
      "metadata": {
        "id": "dZkkJmhR_nZw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User-driven query of interest\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def extract_user_interest(url, interest_query, window=10):\n",
        "  try:\n",
        "    response = requests.get(url, timeout=10)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "      script.extract()\n",
        "\n",
        "    main = soup.find('main') or soup.find('div', {'id':'main'}) or soup.find('div', {'id': 'content'})\n",
        "    text = soup.get_text(separator =' ')\n",
        "    text = re.sub(r'\\s+', ' ', text).lower()\n",
        "\n",
        "    words = text.split()\n",
        "    interest = interest_query.lower()\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "      if re.search(rf'\\b{re.escape(interest)}\\b', word):\n",
        "        context = words[i+1:i+1+window]\n",
        "\n",
        "        cleaned = [\n",
        "            lemmatizer.lemmatize(w)\n",
        "            for w in context\n",
        "            if w not in stop_words and w.isalpha() and len (w) > 2\n",
        "        ]\n",
        "\n",
        "        return cleaned\n",
        "  except Exception as e:\n",
        "    print(f\"Error fetching {url}: {e}\")\n",
        "    return []"
      ],
      "metadata": {
        "id": "QP-C4cuOWU08"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank based on interest\n",
        "def interest_rank(interest_rank_df, interest_col='Interest'):\n",
        "  all_words = [\n",
        "      word\n",
        "      for word_list in df['Interest']\n",
        "      if isinstance(word_list, list)\n",
        "      for word in word_list\n",
        "  ]\n",
        "  word_counts = Counter(all_words)\n",
        "\n",
        "  # Summary df\n",
        "  interest_rank_df = pd.DataFrame(word_counts.items(), columns=['Word', 'Count'])\n",
        "  interest_rank_df.sort_values(by='Count', ascending= False, inplace= True)\n",
        "  interest_rank_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  return interest_rank_df"
      ],
      "metadata": {
        "id": "OgxGOfPCw4Wg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### STREAMLIT BUTTON\n",
        "# Raw query output to streamlit\n",
        "if st.button(\"Run Analysis\"):\n",
        "  df = get_company_data_from_serpapi(company_query, serpapi_key)\n",
        "  df = filter_valid_company_sites(df)\n",
        "\n",
        "  # apply found name and clean the domain name\n",
        "  df['Company'] = df['Domain'].apply(get_company_name)\n",
        "  df['Domain'] = df['Domain'].str.replace(r'\\?.*', '', regex=True)\n",
        "\n",
        "  # Trending words\n",
        "  df['Keywords'] = df['Domain'].apply(lambda url: query_keywords(url, company_query))\n",
        "  keyword_df = analyze_keywords(df)\n",
        "\n",
        "  # Subject of Interest\n",
        "  df['Interest'] = df['Domain'].apply(lambda url: extract_user_interest(url, interest_query))\n",
        "  interest_rank_df = interest_rank(df)\n",
        "\n",
        "  # Streamlit\n",
        "  # Raw query\n",
        "  if not df.empty:\n",
        "    st.subheader(\"Raw Query Table\")\n",
        "    st.dataframe(df)\n",
        "  else:\n",
        "    st.warning(\"Please run a search to populate the table\")\n",
        "\n",
        "  # Keyword found output to streamlit\n",
        "  if not keyword_df.empty:\n",
        "    st.subheader(\"Trending keywords\")\n",
        "    st.dataframe(keyword_df)\n",
        "  else:\n",
        "    st.warning(\"No trending keywords found\")\n",
        "\n",
        "  # Interest ranking output to streamlit\n",
        "  if not interest_rank_df.empty:\n",
        "    st.subheader(f\"Interest dependencies for {interest_query}\")\n",
        "\n",
        "    # Plot the subject of interest for better readibility\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.barh(interest_rank_df['Word'], interest_rank_df['Count'], color='skyblue')\n",
        "    ax.set_xlabel('Count')\n",
        "    ax.set_ylabel('Words')\n",
        "    ax.invert_yaxis()\n",
        "    ax.xaxis.set_major_locator(mticker.MaxNLocator(integer= True))\n",
        "    st.pyplot(fig)\n",
        "  else:\n",
        "    st.warning(\"No dependencies found\")\n",
        "\n",
        "  # Summary panel\n",
        "  st.subheader(\"Summary Panel\")\n",
        "  with st.container():\n",
        "    col1, col2= st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "      st.metric(\"Extracted Company Websites\", len(df))\n",
        "      st.metric(\"Extracted Keywords\", df[\"Keywords\"].apply(bool).sum())\n",
        "\n",
        "    with col2:\n",
        "      total_interest_tags = interest_rank_df['Count'].sum() if not interest_rank_df.empty else 0\n",
        "      top_interest = interest_rank_df.iloc[0]['Word'] if not interest_rank_df.empty else \"N/A\"\n",
        "      st.metric(\"Total Interest Tags\", total_interest_tags)\n",
        "      st.metric(\"Top Interest Tag\", top_interest)\n",
        "\n",
        "  # Export panel\n",
        "  st.subheader(\"Export Data\")\n",
        "  if not df.empty:\n",
        "    output = io.BytesIO()\n",
        "\n",
        "    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
        "      df.to_excel(writer, sheet_name=\"Raw Query Table\", index=False)\n",
        "\n",
        "      if not keyword_df.empty:\n",
        "        keyword_df.to_excel(writer, sheet_name='Trending Keywords', index=False)\n",
        "      else:\n",
        "        pd.DataFrame([\"No Keywords found\"]).to_excel(writer, sheet_name='Trending Keywords', index=False, header=False)\n",
        "\n",
        "      if not interest_rank_df.empty:\n",
        "        interest_rank_df.to_excel(writer, sheet_name='Interest Ranking', index=False)\n",
        "      else:\n",
        "        pd.DataFrame([\"No interest dependencies found\"]).to_excel(writer, sheet_name='Interest Ranking', index=False, header=False)\n",
        "\n",
        "    output.seek(0)\n",
        "    report_name = f\"Scraping {company_query} with subject of interest {interest_query}.xlsx\"\n",
        "\n",
        "    st.download_button(\n",
        "        label=\"Download Report\",\n",
        "        data=output,\n",
        "        file_name=report_name,\n",
        "        mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
        "    )\n",
        "  else:\n",
        "    st.warning(\"There is nothing to download, please rerun\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oQhgPVfVkC3",
        "outputId": "0f6a9730-f7a1-4d64-c782-82510dc8579a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-15 14:30:55.206 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 14:30:55.211 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 14:30:55.213 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 14:30:55.214 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 14:30:55.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Q1hvQAaagBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}